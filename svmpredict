#!/usr/bin/python

import argparse
import os
import csv

import sklearn.svm  as sklsvm
import numpy as np

from sklearn.preprocessing import normalize

def make_parser():

    pars = argparse.ArgumentParser(description='learn data in specified'
                                   ' headed columns for specified trials'
                                   ' and check for accuracy on remaining'
                                   ' trials -- predicting specified data')
    pars.add_argument('data', help='file or folder of data to be used, if'
                      ' folder, each file should be separate instance of data')
    pars.add_argument('-n', '--number', help='number of entries to train on the'
                      ' the rest will be predicted (if number is less than 1,'
                      ' will treat as percentage of total entries', type=float,
                      default=0.5)
    pars.add_argument('-l', '--learn', help='the heads of the columns to be'
                      ' used for learning', nargs='+')
    pars.add_argument('-p', '--predict', help='the head of the column to be'
                      ' predicted') 
    pars.add_argument('-d', '--delimiter', help='delimiter for the data files',
                      type=str, default='infer')
    pars.add_argument('-f', '--filter', help='filter row with head (first '
                      ' string) and only take values matching following'
                      ' strings', nargs='+', default=False)
    return pars

def get_list_of_files(input_given):
    
    if os.path.isdir(input_given):
        files = [os.path.join(input_given, x) for x in os.listdir(input_given)
                 if os.path.isfile(x)]
    elif os.path.isfile(input_given):
        files = [input_given]
    else:
        raise IOError('path given is neither folder nor file')

    return files

def lowerify_list(strings):    
    return [x.lower() for x in strings]

def get_relevant_data(file_, delim):

    learn_data = []
    predict_data = []
    read = csv.reader(file_, delimiter=delim)
    for j, row in enumerate(read):
        if j == 0:
            headers = lowerify_list(row)
            learn_indices = [headers.index(l) for l in args.learn]
            predict_index = headers.index(args.predict)
            if args.filter:
                filter_index = headers.index(args.filter[0])
                filter_look = args.filter[1:]
        elif not args.filter or row[filter_index] in filter_look:
            learn_data.append([float(row[x]) for x in learn_indices])
            predict_data.append(int(row[predict_index]))

    learn_data = np.array(learn_data)
    predict_data = np.array(predict_data) 
    predict_data = predict_data.reshape(predict_data.shape[0], 1)

    return learn_data, predict_data

def split_data(learn, predict, number):
    combined = np.concatenate((learn, predict), axis=1)
    if number < 1:
        number = int(combined.shape[0] * number)
    np.random.shuffle(combined)
    learn_set = combined[:number]
    predict_set = combined[number:]
    
    return learn_set, predict_set

def fit_model(learning):
    data = learning[:, :-1]
    truth = learning[:, -1]
    model = sklsvm.SVC()
    model = model.fit(data, truth)

    return model

def classify_data(model, predict):
    data = predict[:, :-1]
    truth = predict[:, -1]
    guesses = model.predict(data)
    right = np.sum(guesses == truth)

    print truth.shape
    
    return float(right) / truth.shape[0]

if __name__ == '__main__':

    parser = make_parser()
    args = parser.parse_args()
    
    args.learn = lowerify_list(args.learn)
    args.predict = args.predict.lower()

    files = get_list_of_files(args.data)

    for i, f in enumerate(files):
        with open(f, 'rb') as file_:
            if i == 0 and args.delimiter == 'infer':
                sniffums = csv.Sniffer()
                dialect = sniffums.sniff(file_.read(1024))
                delim = dialect.delimiter
            else:
                if args.delimiter == 'tab':
                    delim = '\t'
                else:
                    delim = args.delimiter
            
            learn_data, predict_data = get_relevant_data(file_, delim)

        if learn_data.shape[0] > 50:
            learn_data = normalize(learn_data)
            training, predicting = split_data(learn_data, predict_data, 
                                              args.number)
            model = fit_model(training)
            accuracy = classify_data(model, predicting)
            
            print accuracy
